# network layer settings

class Settings:
    layers = 3
    layout = [5, 8, 3]
    activation = "relu" # relu, sigmoid, tanh
    alpha = 0.5 # learning rate 0-1
    decay = 0.3 # decay rate 0-1
    batches = 200